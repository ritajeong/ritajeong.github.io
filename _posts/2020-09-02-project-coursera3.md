---
layout: post
title: "Coursera Machine Learning Study Week3"
subtitle: ""
categories: project
tags: project
comments: true
date: 2020-09-02 20:13:00 -0400
---


# Classification and Representation
## Classification
- 학습목표 : 우선은 0과 1 두 가지의 값만을 갖는 분류 문제를 먼저 다루도록 하고 이후에 0, 1, 2, 3 등 여러가지 y 값을 찾는 분류 문제도 다룰 수 있도록 하겠습니다.
이와 같은 문제를 다중분류 (multiclass classification) 문제라고 하죠.
 
 - 선형 회귀를 분류 문제에 적용하는 건 대부분의 경우에서 좋은 생각이 아닙니다.    
 이 첫번째 예시에서, 제가 데이터를 추가하기 이전의 선형 회귀는 운좋게 우리에게 특정 예시들에 대해 잘 작동하는 가설을 주었지만 일반적으로 선형회귀를 (분류) 데이터에 적용하면, 운이 좋을 수도 있지만 대부분은 잘못될 것입니다.   
 그래서 저라면 분류 문제들에 대해 선형 회귀를 사용하지 않을 겁니다. 

- 앞으로의 강의에서는 로지스틱 회귀라 불리는 알고리즘을 공부할 건데요, 로지스틱 회귀의 결과값, 예측값은 항상 0에서 1사이이며 1보다 크거나 0보다 작은 값을 가질 수 없습니다.   


## Hypothesis Representation
- 학습목표 : 로지스틱 회귀분석을 시작하면서 여러분들에게 표현 모델을 보여주고 싶습니다. 그것은, 우리가 분류문제를 가지고 있을 때에 우리의 가설을 표현하기 위해 사용할 함수입니다.    

- 이전에, 우리는 분류문제의 결과값이 0에서 1 사이가 되도록 한다고 말했습니다.     그래서 우리는 이러한 결과값을 만족하는가설 즉, 예측값이 0에서 1사이인 가설을 만들고 싶습니다. 선형 회귀분석을 사용할 때에는, 이것이 가설 형식이었습니다. h(x)가 Θ의 전치행렬 * x입니다. 로지스틱 회귀분석에서는, 전 이것을 조금 변경하여 g(Θ의 전치행렬 * x)으로 다음과 같은 g함수를 사용하여 변경합니다. G(z) 중, z는 실수이며, 1 더하기 -z 지수의 e 분의 1로 정합니다.

- 이것은 시그모이드 함수 혹은 로지스틱 함수이며 로지스틱 함수라는 용어는 로지스틱 회귀분석이라는 이름의 만들었습니다. 이 외에도, 시그모이드라는 용어나 로지스틱 함수라는 용어는 기본적으로 동의어이며 같은 뜻을 의미합니다. 

## Decision Boundary
- 학습목표 :  we talked about the hypothesis representation for logistic regression.
we know that y equals one is more likely, that is the probability that y equals one is greater than or equal to 0.5, whenever theta transpose x is greater than zero. And this formula that I just underlined, -3 + x1 + x2, is, of course, theta transpose x when theta is equal to this value of the parameters that we just chose.
- 선형경계가 생기는 경우와 그렇지 않은 경우가 존재한다. 
decision boundeary / non-linear decision boundaries.
So, with these visualizations, I hope that gives you a sense of what's the range of hypothesis functions we can represent using the representation that we have for logistic regression.

# Losixtic Regression Model
## Cost Function
- 학습 목표 : 로지스틱 회귀(logistic regression)에서 매개변수 θ를 어떻게 fitting하는지 알아보도록 하죠. 특히, 매개변수 θ를 fitting하기 위해서 우리가 최적화할 목표물, 즉 비용함수가 무엇인지 정의하려고 합니다.
- 이 비용 함수는 선형 회귀를 다룰 때에는 문제가 없었어요. 그치만, 우리가 지금 하고 있는 건 로지스틱 회귀입니다. 만약 여기에 J를 대입하고, 이 비용 함수를 최소화할 수 있다면 그럼 문제가 없을 거예요. 하지만 우리가 여기 이 비용 함수를 사용한다면 이 비용 함수는 매개변수 θ에 대해 볼록 함수가 아니라고 밝혀졌어요. 볼록 함수가 아니라고 한 게 무슨 뜻이냐면, 우리에게 어떤 비용함수 J(θ)가 있어서 로지스틱 회귀 문제를 다룰 때 이 함수 h가 비선형성을 가져서, 즉, 1/(1+e^(-θT)x)가 됩니다. 즉 이건 꽤 복잡한 비선형 함수입니다. 그리고 이 sigmoid함수를 다시 이 안에 대입하고, 또 이 비용 함수를 여기에 대입한다면 J(θ)는 이런 모양이 됩니다. 여기 보이는 바와 같이 J(θ)는 이렇게 많은 극소점을 가진 함수가 되지요. 이런 함수를 수학적 용어로 비볼록함수라고 합니다. 그리고 이런 비볼록함수의 경우 경사하강법을 적용해도 전체 함수의 최소값에 도달한다는 보장이 없습니다. 반면에 비용함수 J(θ)가 볼록함수라고 한다면 이 함수는 활 모양으로 한 번만 구부러진 함수이고, θ에 대해 경사하강법을 적용함으로서 수렴되는 값은 반드시 전체 함수의 최소값이라고 할 수 있습니다. 그러므로 이 제곱된 비용 함수를 사용함으로 인해 생기는 문제점은 이 가운데 있는 게 비선형적인 함수이기 때문에 J(θ)가 비볼록함수가 된다는 겁니다. 
 - 그래서 우리가 하고 싶은 건, 다른 비용 함수를 제시하는 겁니다. 볼록한 비용 함수를요. 그래서 경사하강법 같은 좋은 알고리즘을 적용할 수 있도록 하고, 전체 함수의 최소값을 찾을 수 있도록 하는 거죠. 
- 이제 로지스틱 회귀에서 사용할 비용함수를 살펴보겠습니다.우리가 이야기할 것은 비용, 즉 알고리즘이 지불해야 할 값에 대한 겁니다. 알고리즘이 hθ(x)값을 넘어섰을 때요. 그래서 예를 들어 이게 0.7같은 숫자라면, 그건 hθ(x)값을 예측하고 있습니다. 그리고 실제 비용을 가리키는 레이블은 y가 됩니다. 이 비용은 y=1일 때 -log(hθ(x))이고요, y=0일 때 -log(1-hθ(x))입니다. 이거 좀 복잡해 보이죠? 하지만 이 함수를 그리면서 이게 뭘 하는지 직감적으로 알아봅시다. 먼저 y=1일 때 입니다. 만약 y=1이라면, 비용함수는 -log(hθ(x))입니다. 이걸 플롯해보면, 여기 x축을 h(x)라고 합시다. 그러면 우리가 알고 있는 대로 가설 h(x)는 0 아니면 1이잖아요, 맞죠, 그래서 h(x)는 0에서 1까지만 값을 가집니다. 이 비용함수가 어떻게 생겼는지 플롯해 보면 이렇게 된다는 걸 알 수 있어요.
 플러스마이너스를 뒤집으면 이게 -log(z)예요. 그리고 우리가 알고 싶은 건 이 함수 중에서 0과 1 사이가 어떻게 생겼는지이므로, 나머지를 지웁시다. 그러면 남은 게, 이제 알겠죠. 이 부분의 곡선이에요. 그리고 이게 바로 여기 왼쪽의 곡선입니다. 이제 이 비용함수는 몇 가지 재미있고 바람직한 속성을 가지고 있어요. 첫째로, 잘 보면 y=1인 경우 h(x)=1이 됩니다. 다시 말하면, 가설을 정확하계 예측한 경우 h=1이고 y가 가설이 예측한 값과 맞아떨어진다면 비용은 0이 됩니다. 맞죠? 이건 곡선이 평평해지지 않는다는 사실과 부합합니다. 커브는 계속 꺾어지게 됩니다. 다시 해 보죠, h(x)=1일 경우에 주목한다면 이 가설은 y=1일 것을 예측하고, 실제로 y=1이라면 비용은 0이 됩니다. 그건 바로 여기 이 점에 해당합니다. 맞죠? 만약 h(x)=1이라면 우리가 생각할 수 있는 y=1인 경우는 여기 한군데입니다. 그런데 만약에 h(x)=1이고 비용이 여기라면 이건 0가 됩니다. 그리고 이건 우리가 원하던 건데요, 왜냐하면 우리가 결과값 y를 정확히 예측했다면 비용은 0이기 때문이죠. 하지만 여기서 하나 더 알게 된 점은, 여기 h(x)가 0에 가까워지면 그래서 가설의 출력값이 0에 가까워지면, 비용은 증대하고 무한대로 가 버린다는 겁니다. 그리고 이게 무슨 역할을 하냐면, 가설 hθ(x)=0, 즉 y=1일 확률이 0일 때를 직관적으로 포착하게 해 줍니다.

- 예시로 이해하기
예를 들어 당신은 의사이고 환자에게 이렇게 말했습니다. "당신이 악성 종양을 가지고 있을 확률, 즉 y=1일 확률은 0입니다." "그래서 당신의 종양이 악성이라는 건 절대적으로 불가능해요."하지만 이런 경우도 있죠. 검사를 해 보니 그 종양, 그 환자의 종양이 악성이었습니다. 그래서 만약 y=1이라면, 벌써 환자에게 말했는데, '악성일 확률은 0이에요' 하고. '그래서 악성 종양이라는 건 불가능해요' 하고 말했는데, 만약 우리가 환자에게 그렇게 확신을 가지고 말했는데 결국엔 틀렸다면 이 학습 알고리즘을 처벌해야 하죠. 엄청난 비용을 부과시켜야 해요. 그게 여기에 포착된 비용의 값입니다. 만약에라도 y=1이라는 값이 나오면 비용은 무한대이고 h(x)=0이 됩니다. 

## Simplified Cost Function and Gradient Descent
- 학습목표
1. 지금까지 사용해 왔던 비용 함수를 조금 더 간단하게 쓰는 방법을 알아보겠습니다. 
2. 그리고 또 어떻게 경사하강법을 적용해서 로지스틱 회귀의 매개변수를 피팅할건지도 알아볼 겁니다. 
3. 이 강의의 마지막에서 여러분은 로지스틱 회귀가 완벽하게 작동하는 버젼을 어떻게 구현하는지를 알게 될 겁니다.
- y가 0이나 1의 값이기 때문에 우리는 비용 함수를 작성하는 더 간단한 방법을 생각할 수 있습니다. 특히, 비용 함수를 여기의 두 줄처럼 y=1 또는 y=0이라는 두가지 케이스로 나누어서 적는 대신에 저는 이 두 줄을 하나의 방정식으로 압축하는 방법을 보여드리겠습니다. 그리고 이것은 비용 함수를 작성하고 기울기 강하를 유도하는 것이 더 편리 할 것입니다. 구체적으로 다음과 같이 비용 함수를 쓸 수있습니다. 저는 이제 cost(h(x), y)를 -y*log(h(x)) -(1-y)* log(1-h(x))라고 적겠습니다. 간단하게 설명하자면 이 표현은, 아니,이 식은 우리가 위에 적은 비용함수의 정의와 동일하며 함축된 방법입니다. 
- 다시 한번 말씀드리자면, 제 hypothesis의 결과를 입력 x와 매개 변수 θ가 주어졌을 때 y가 1과 같을 확률로 해석할 것입니다. 하지만, 당신은 y가 1과 같을 확률을 추정하는 것을 단지 나의 가설이라고만 생각할 수 있습니다. 그래서 우리가 해야 할 것은 우리의 training set에 최적하기 위한 매개 변수인 θ의 함수인 J(θ)를 어떻게 최소화 할 것인가 입니다. 우리는 비용 함수를 최소화하기 위해서 경사 하강법을 사용할 것 입니다.
- 경사 하강법으로 로지스틱 회귀를 구현할 때, 우리는 θ0부터 θn까지 모두 다른 매개 변수 값을 가지고 있으며, 이 식을 이용해서 θ를 업데이트 해야 합니다. 우리가 할 수 있는 한가지는 for loop입니다. i=0부터 n까지나, i=1부터 (n+1)까지 for loop를 사용해서 우리는 이 각각의 매개 변수들을 업데이트 해야합니다. 물론 for 루프를 사용하는 것이 아니라, vector rise implementation을 사용하는 것이 이상적입니다. vector rise implementation은 m+1개의 매개 변수를 한번에 업데이트할 수 있습니다. 여러분이 스스로 이해했는지 확인하기 위해서, vector rise implementation을 이 알고리즘에 수행하는 방법을 확인할 수 있습니다. 
- 로지스틱 회귀를 위한 하강 기울기 알고리즘에 대해 알게 되었습니다. 우리가 이전 선형 회귀에 사용된 마지막 아이디어는 feature scaling입니다. 우리는 feature scaling이 어떻게 선형 회귀의 경사 하강법을 빠르게 수렴하게 했는지 봤습니다. 선형 회귀입니다. feature scaling의 아이디어를 로지스틱 회귀의 경사 하강법에도 적용해 보겠습니다. 로지스틱 회귀입니다. 우리는 매우 다른 범위를 가지는 feature들을 가지고 있다면, feature scaling을 적용하여 로지스틱 회귀에서 경사 하강법을 빠르게 만들어 보겠습니다.
## Advanced Optimization
- 학습목표 : we talked about gradient descent for minimizing the cost function J of theta for logistic regression. I'd like to tell you about some advanced optimization algorithms and some advanced optimization concepts.
- So another way of thinking about gradient descent is that we need to supply code to compute J of theta and these derivatives, and then these get plugged into gradient descents, which can then try to minimize the function for us. 
- But gradient descent isn't the only algorithm we can use. And there are other algorithms, more advanced, more sophisticated ones, that, if we only provide them a way to compute these two things, then these are different approaches to optimize the cost function for us. So conjugate gradient BFGS and L-BFGS are examples of more sophisticated optimization algorithms that need a way to compute J of theta, and need a way to compute the derivatives, and can then use more sophisticated strategies than gradient descent to minimize the cost function.
- 장점 : No need to manually pick alpha. Often faster than gradient descent.
- 단점 : More complex.
- 다른 알고리즘들은 직접 구현하기보다 라이브러리를 사용하는 것을 추천. 

## Multiclass Classification: One-vs-all 
- 학습목표 : 멀티클래스 분류 문제에 대해 로지스틱 회귀를 어떻게 적용하는지,  특히 'one-vs-all'분류라고 불리는 알고리즘에 대해 배운다.   
- 예시 1 : 취미 이메일에 네 개의 클래스로 분류      
- 예시 2 : 의학적 진단. 감기 1 독감2 등등으로 분류  
- 예시 3 : 날씨 분류    
이런게 멀티클래스 분류 문제고, 인덱싱을 0부터 or 1부터인지는 중요하지 않음  

<img src = "\assets\img\posts\20200920.jpg" >
이전의 바이너리 분류는 그래프에서 양 그룹을 나누었음.   
반면에 멀티클래스 분류 문제의 경우, 데이터 집합은 다음과 같이 세 개의 다른 기호를 사용하여  
서로 다른 세 개의 클래스로 표현됩니다.  
<img src = "\assets\img\posts\20200920-1.jpg" >
<br>

주어진 데이터에 어떻게 학습 알고리즘을 적용하는가?
- 이미 회귀를 사용해 두 개의 데이터로 분류하는 법을 배웠음. 
양, 음 클래스로 구분하는 것처럼     
처음에는 클래스 1을 양으로 두고 나머지를 음으로 두어서 분류함.
두번째 스텝에서는 클래스 2를 양으로, 나머지는 음으로 분류.
이런 식으로 반복해서 one vs all분류를 할 수 있다.

- 그리고 표준 로지스틱 회귀 분류를 훈련시킴.    
바이너리 분류에서와 같이 경계선이 그어진다. 
이게 세 가지 분류에 대한 fitting이다.   

- 우리가 한 것은 세 개의 로지스틱 회귀 h(i)(x)를 훈련시켜서   
각각이 클래스 i에 대해 y=i일 확률을 예측하도록 했습니다.    

- 마지막으로, 새로운 입력값 x가 주어진다면  
예측을 하기 위해서 우리가 해야 할 작업은    
그냥 세 개의 h를 새로운 x에 대해 전부 돌려 보고  
최대값이 나온 클래스 i를 고르면 됩니다.     
기본적으로는 그냥 h를 고르면 되는데요   
세 개의 h 중 가장 신뢰도가 높고 가장 열정적으로 "이것이 올바른 클래스다"
라고 말하는 것을 선택합니다.    
가장 높은 확률을 주는 i 값이라면 y가 그 값이라고 예측할 수 있겠죠.  
여기까지가 one-vs-all을 사용한 멀티 클래스 분류였고요, 이 간단한 방법을 사용하면 이제 로지스틱 회귀를 사용하여 다중 클래스 분류 문제도 해결할 수 있습니다.


## The problem of Overfitting
- 학습목표 : **과적합 overfitting**이 무엇인지, **정규화 regularization**를 배움.   
- 부연설명 : 로지스틱 회귀(logistic regression)와 선형회귀(linear regression)는 많은 머신러닝 문제들에 적용될 수 있지만 과적합 문제에 빠져 성능이 잘 안 나올 수 있다. 그래서 정규화를 배움. 
- 예시 : 집값예측 문제. 선형함수    
<img src="\assets\img\posts\20200920-2.jpg">
<br>

- 첫 번째 그래프 설명 :     
이 문제를 풀 수 있는 방법으로는 선형 함수(1차 함수, 직선)를 데이터에 맞게끔 하는 것이고     
이 문제를 해결하고나면 데이터를 가로지르는 선을 볼 수 있겠죠    
그러나 이 모델은 별로 좋지 못한 것 같습니다     
데이터를 보면 집값이 커지면서 가격이 올라가는게 보이지만
__집의 사이즈가 커질수록 집값이 천천히 올라 결국 평탄화 되죠__
그러다 보니 우리의 알고리즘은 이 데이터에 잘 들어맞지 않습니다      
이런 문제를 **과소적합(underfitting)** 이라고 하고       
다른 말로는 알고리즘이 **high bias(높은 편향)** 을 갖고 있다고 합니다        
__이 2가지 용어는 모두 대략적으로 우리의 모델이 데이터에 제대로 맞지 않는다는 것을 의미__
이 용어는 역사적으로 혹은 기술적으로 정해진 용어입니다  
그러나 둘다 표현하고자 하는 아이디어는 직선을 데이터에 맞추는 경우  
이런 경우에 우리는 알고리즘이 강한 **선입견(preconception)** 을 갖고 있다 혹은 강한 **편향(bias)** 를 갖고 있다라고 얘기하고  
집 값이 집 크기에 따라서 데이터의 분포와 상관없이   
선형적으로 변화할 것이라는 것입니다     
데이터가 예측에 반대하는 증거를 보여줌에도 불구하고     
데이터를 직선에 맞추려다보니 **편향(bias)** 혹은 **선입견(preconception)**이 생기고         
따라서 형편없는 결과가 나옵니다 
- 두번째 그래프 설명 :  
그러면 여기 가운데 처럼 2차 함수를 끼워넣을 수도 있겠죠         
이런 데이터에 2차 함수를 넣으면 이렇 모양의 커브를 얻을 수 있겠죠   
그리고 이건 꽤 잘 맞습니다 
- 세번째 그래프 설명 :  
그리고 마지막으로 좀 극단적인 예제를 보면       
4차 함수를 데이터에 맞추고 있습니다    
그래서 여기는 세타0 부터 세타5까지 5개의 파라미터를 이용해 5개의 데이터 모두를 선을 이용해 맞출 수 있습니다    그러면 이런 선을 얻을 수 있겠죠    
한편으로 보면 이 선은 데이터에 잘 맞으니 적어도 훈련용 데이터에 한해서는 꽤 좋은 성능을 낸다고 볼 수 있죠  
근데 보면 엄청나게 꼬아져 있는 선이죠?  
이 선을 보고 우리는 이 선이 제대로 된 집 값을 예측할거라고 생각되지 않습니다    
이런 문제를 보고 **과적합(overfitting)**이라고 합니다   
그리고 이걸 다른 말로 알고리즘이 **high variance(높은 분산)**을 갖고 있다고 합니다  
high variance도 역사적 혹은 기술적으로 탄생한 용어입니다    
어쨋건 직관적으로 우리는 어떤 고차 함수를 데이터에 맞추려고 하고    
사실 완벽히 맞는 함수를 만들고자 하면 만들 수 있지만 그렇게 되면 커다란 변동성을 갖게 되죠  
그리고 우리는 저런 변동성을 막아줄 엄청난 데이터를 가지고 있지 않죠     
이런 상황을 보고 **과적합(overfitting)**이라고 합니다   
여기 가운데 같은 경우는 어떤 이름이 따로 있는 것은 아니고 그냥 잘 맞는다라고 표현합니다     
2차 함수가 데이터에 딱 잘 맞고 있죠     
- 정리 : **과적합 문제**는 많은 특성(feature)들이 존재할 때     
우리의 가설이 __학습용 데이터에만 잘 맞아서__   
학습용 데이터를 대상으로는 비용함수가 거의 0에 가까운 값을 혹은 0이 나오고  
대신에 엄청 복잡한 커브로 학습 데이터에 맞춰질 것이고   
학습 데이터를 제외한 __다른 새로운 예제가 들어올 경우__     
새로운 예제에 대한 일반화(generalized)된 예측은     
제대로 하지 못하는 경우가 발생합니다    
일반화(generalized)라는 용어는 가설이 새로운 데이터에도 얼마나 잘 맞냐라고 생각하면 됩니다  
여기서 새로운 데이터는라는 것은 학습용 데이터에 없던 집 크기에 대한 집 가격 정보겠죠    
우리는 앞의 슬라이드에서 **선형 회귀에 대한 과적합 케이스**를 봤는데요  
- 이것은 **로지스틱 회귀**에도 마찬가지입니다   
여기 특성 x1, x2에 대한 로지스틱 회귀 예제가 있습니다   
한가지 방법으로는 이렇게 __간단한 가설(hypothesis)를 로지스틱 회귀__에 사용할 수 있습니다   
여기 g는 시그모이드 함수입니다  
이런 가설을 가지고 로지스틱 회귀를 하면 이런 간단한 선으로 양성(positive)과 음성(negative)를 나누겠죠   
그러나 이건 그다지 잘 맞아 떨어지지 않는거 같습니다     
다시 말하자면 이런 케이스는 **과소적합(underfitting)** 혹은 **high bias(높은 편향)**      
좀 더 나아가서 좀 더 고차의 특성들을 가설에 추가적으로 이용하면 이런 decision boundary를 얻을 수 있겠죠     
보다시피 좀더 데이터에 잘 맞는걸 볼 수 있죠     
아마도 이 선이 학습용 데이터로부터 얻을 수 있는 최적일거 같네요     
그리고 마지막으로 극단적인 예제가 있죠  
엄청 나게 많은 고차항을 생성하여 엄청 높은 차수의 다항식을 만들면   
로지스틱 회귀는 데이터에 맞는 엄청 자잘자잘하게 꼬아진 decision boundary를 구해내게 될 것입니다     
자신의 예제를 모두 맞추기 위해 엄청 길게 꼬여있겠죠     
만약에 특성 x1과 x2가 암이 악성인지 아닌지를 판단하는데 쓰인다고 하면   
이 가설은 정말 좋지 못한 예측을 하는 것이죠     
다시 말하지만 이런 케이스를 과적합이라고 하고 **high varicance(높은 분산)**이라고도 합니다  
그리고 새로운 예제에 잘 일반화되지 않습니다     

- 이 코스의 뒤쪽에서는 **학습 알고리즘이 과적합인지 디버깅과 분석하는 툴**에 대해서 알아볼 것이고   
마찬가지로 **과소적합**에 대해서도 할 것입니다  
일단 과적합 케이스의 문제가 있다고 가정하고     
어떻게 이게 과적합인지 알 수 있을까요?  
우리의 예제들은 모두 1차원 혹은 2차원이었으므로     
그냥 그래프를 그려 무슨 일이 일어나는지 보면서  
__적당한 차원의 다항식을 선택하면 됩니다__  
앞에서 본 집 값 문제를 보면 우리의 가설을 바로 그래프로 그려볼 수 있죠  
- 그려면 다음과 같이 적당히 꼬불꼬불한 선이 우리의 데이터를 지나가겠죠    
우리는 이런 그래프를 보고 적당한 수준의 다항식을 선택할 수 있죠 
따라서 가설을 그래프로 표현하여 어떤 다항식을 사용하는지 보는 것은 한가지 방법이 되겠죠         
__하지만 이 방법은 항상 동작하지 않습니다__         
__다항식의 차수가 높은 문제가 아니더라도 엄청나게 많은 특성이 있는 경우에는__   
다항식의 차수를 선택할 여지조차 없죠    
게다가 특성이 많아질수록 그래프를 통해 시각적으로 표현하기가 더욱 어려워집니다  
그래서 어떤 특성을 남겨야 할지 선택하기 어렵죠  
- 구체적으로 얘기하면 집 값을 예측할건데 __특성이 엄청 많은 경우__ 입니다   
게다가 특성 하나하나가 예측에 도움이 될 것 같은 경우죠  
이렇게 특성이 많은데 학습 데이터가 적은 경우 과적합 문제가 발생할 수 있습니다    
- 과적합 문제 해결법 : 과적합 문제를 해결하기 위해서는 크게 **2가지 해결법**이 있습니다     
첫번째로는 특성의 갯수를 줄이는 방법입니다  
구체적으로 얘기하면 특성들을 쭉 보고 이거는 쓸만해 이거는 안쓸만해 해서     
어떤 특성은 남기고 어떤 특성은 버리는 식으로 제거할 수 있겠죠   
- 이 코스의 뒤쪽에서는 **모델 선택 알고리즘**을 배울겁니다  
알고리즘이 어떤 특성을 사용할 건지 자동으로 선택하거나 버릴지 선택하지요    
여튼 특성의 수를 줄이는 것이 과적합 문제에 해결책으로 이용될 수 있습니다    
모델 선택 알고리즘을 애기하면서 여기에 대해 좀더 자세히 얘기하도록 하겠습니다   
그러나 __단점으로는 몇 가지 특성을 버림으로 인해 문제에 포함된 정보를 같이 버리게 된다는 것입니다__     
예를 들어 모든 특성들이 집 값을 예측하는데 도움이 되는경우      
우리는 그 어떤 정보도 버리고 싶지 않을겁니다    
- 두번째 해결법으로는 **정규화(regularization)**이 있습니다     
뒤의 영상들에서 배울겁니다 모든 특성들을 남기되     
각각의 특성이 갖는 영향 규모를 줄이는 겁니다    
다르게 얘기하면 세타값이 미치는 영향을 줄이는 거죠  
이 방법은 과적합 문제를 잘 해결해줍니다     
엄청 많은 특성들이 있을 때  
그리고 각 특성들이 예측에 엄청 작은 영향을 미치는 경우의 문제가 있 다고 가정해봅시다     
앞서 얘기했던 집값 예측 같은거 말이죠 엄청 많은 특성이 있고     
각각의 특성이 예측값에 영향을 주죠 그래서 특성을 버리기 싫은겁니다  

- 이로써 정규화에 대한 큰 그림을 설명드렸습니다     
아직은 어떤 의미인지 잘 와닿지 않을 수 있습니다     
그러나 다음 영상에서 __수학적으로 정규화가 어떻게 동작하는지 또 어떻게 적용하는지__ 알아보겠습니다  
그 다음에 __정규화를 어떻게 사용해야 우리의 학습 알고리즘이 좋아지고 과적합을 피할 수 있는지__ 배우겠습니다     


## Cost Function
- 학습목표 : 정규화를 위한 비용함수
- 왼쪽 그래프의 가설함수는 적절함. 오른쪽은 과적합  
세타 3,4를 패널티를 줘서 작은 값으로 만든다고 가정하자. 
세타 3,4를 0에 가깝게 만들면 4차함수가 왼쪽처럼 2차함수와 거의 같아지므로 가설함수가 적절해진다.    
기존의 비용함수에 세타3과 세타4를 더해 비용함수를 수정해보자.   
기존과 다른 점은 : 세타3과 4를 최소화해서 각각 0에 가깝게 만든다.
- Regularization
특정 세타값이 아닌, 모든 세타값을 작게 만들면 가설함수는 더 단순화되는 효과가 있다. 
따라서 과적합이 발생할 가능성이 줄어듬. 
세타 3,4를 0에 가깝게 만든 것이 하나의 예시이다.    
- Housing : 입력값이 100개가 있다면, 어떤 값을 줄어들어야할 지 미리 알지 못함.  
같은 의미로, 어떤 세타를 줄여야할지 모른다. 
기존의 비용함수를 모든 세타값을 고려해서 수식과 같이 변경한다.
모든 세타값을 수식에 넣은 이유는 어떤 세타값을 줄여야할지 모르기 때문.
관습상 i=1부터 시작하기 때문에 세타0은 포함하지 않음.
- Regularization : 새로운 비용함수 J(세타) 를 보자. 
목표는 새로운 기술함수를 피팅하는 것이다. 
핑크색 수식의 목표는 파라미터 값을 최소화해서 가설함수를 단순하게 만들고, 과적합을 방지하는 것이다. 
람다는 정규화 파라미터라고 부른다.  
따라서 추상화해서 생각하자면, 기존의 가설함수가 파란색과 같은 굴곡이 많은 그래프였다면 이를 핑크색 수식의 도움으로 2차원 함수 그래프에 가깝게 바꿔서 과적합을 방지하는 것이다. 
- In regularized linear regression, we choose theta to minimize J(theta)
람다값이 매우 커져 세타0을 제외한 모든 세타값이 0에 가까워지면, h(x) = theta0이 가설함수가 된다.
이 가설함수는 부적합이다. underfitting

## Regularized Linear Regression
- cost function
- Gradient descent
- Normal equation
- Non-invertibility(optinal/advanced) : 역행렬을 구하지 못하는 경우

## Regularized Logistic Regression
- 학습목표 : 로지스틱 회귀를 해결하기 위한 2가지 알고리즘(경사하강법, advanced optimization)에 정규화를 적용하는 법을 배운다.
- Cost function
- Gradient descent
- Advanced optimization
